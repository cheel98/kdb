#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥è¯†åº“æ¼”ç¤ºè„šæœ¬

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†åŸºäºdocsç›®å½•æ„å»ºçš„æœ¬åœ°çŸ¥è¯†åº“çš„åŸºæœ¬åŠŸèƒ½ã€‚
"""

import os
import json
from pathlib import Path
import re

def load_all_documents(docs_path="./docs"):
    """åŠ è½½æ‰€æœ‰markdownæ–‡æ¡£"""
    docs_path = Path(docs_path)
    documents = []
    
    if not docs_path.exists():
        print(f"âŒ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {docs_path}")
        return documents
    
    print(f"ğŸ“š æ­£åœ¨æ‰«ææ–‡æ¡£ç›®å½•: {docs_path}")
    
    # é€’å½’æŸ¥æ‰¾æ‰€æœ‰.mdæ–‡ä»¶
    md_files = list(docs_path.glob("**/*.md"))
    
    for file_path in md_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            relative_path = file_path.relative_to(docs_path)
            
            documents.append({
                'path': str(relative_path),
                'full_path': str(file_path),
                'content': content,
                'size': len(content),
                'lines': len(content.split('\n'))
            })
            
        except Exception as e:
            print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
    
    return documents

def analyze_documents(documents):
    """åˆ†ææ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
    if not documents:
        return {}
    
    total_size = sum(doc['size'] for doc in documents)
    total_lines = sum(doc['lines'] for doc in documents)
    
    # æŒ‰ç›®å½•åˆ†ç»„
    dir_stats = {}
    for doc in documents:
        dir_name = str(Path(doc['path']).parent)
        if dir_name not in dir_stats:
            dir_stats[dir_name] = {'count': 0, 'size': 0}
        dir_stats[dir_name]['count'] += 1
        dir_stats[dir_name]['size'] += doc['size']
    
    # æ‰¾å‡ºæœ€å¤§çš„æ–‡æ¡£
    largest_doc = max(documents, key=lambda x: x['size'])
    
    return {
        'total_docs': len(documents),
        'total_size': total_size,
        'total_lines': total_lines,
        'avg_size': total_size // len(documents),
        'directories': dir_stats,
        'largest_doc': largest_doc
    }

def simple_search(documents, query, max_results=5):
    """ç®€å•çš„æ–‡æ¡£æœç´¢"""
    query_lower = query.lower()
    results = []
    
    for doc in documents:
        content_lower = doc['content'].lower()
        
        # è®¡ç®—åŒ¹é…åº¦
        matches = content_lower.count(query_lower)
        if matches > 0:
            # æå–åŒ…å«æŸ¥è¯¢è¯çš„ç‰‡æ®µ
            snippets = []
            lines = doc['content'].split('\n')
            
            for i, line in enumerate(lines):
                if query_lower in line.lower():
                    # è·å–ä¸Šä¸‹æ–‡ï¼ˆå‰åå„1è¡Œï¼‰
                    start = max(0, i-1)
                    end = min(len(lines), i+2)
                    context = '\n'.join(lines[start:end])
                    snippets.append(context.strip())
                    
                    if len(snippets) >= 3:  # æœ€å¤š3ä¸ªç‰‡æ®µ
                        break
            
            results.append({
                'path': doc['path'],
                'matches': matches,
                'snippets': snippets,
                'size': doc['size']
            })
    
    # æŒ‰åŒ¹é…æ•°æ’åº
    results.sort(key=lambda x: x['matches'], reverse=True)
    return results[:max_results]

def show_document_tree(documents):
    """æ˜¾ç¤ºæ–‡æ¡£æ ‘ç»“æ„"""
    print("\nğŸ“ æ–‡æ¡£ç»“æ„:")
    print("=" * 50)
    
    # æŒ‰è·¯å¾„åˆ†ç»„
    tree = {}
    for doc in documents:
        path_parts = Path(doc['path']).parts
        current = tree
        
        for part in path_parts[:-1]:  # ç›®å½•éƒ¨åˆ†
            if part not in current:
                current[part] = {}
            current = current[part]
        
        # æ–‡ä»¶éƒ¨åˆ†
        filename = path_parts[-1]
        current[filename] = doc
    
    def print_tree(node, prefix="", is_last=True):
        items = list(node.items())
        for i, (name, content) in enumerate(items):
            is_last_item = i == len(items) - 1
            current_prefix = "â””â”€â”€ " if is_last_item else "â”œâ”€â”€ "
            
            if isinstance(content, dict) and 'content' in content:
                # è¿™æ˜¯ä¸€ä¸ªæ–‡ä»¶
                print(f"{prefix}{current_prefix}ğŸ“„ {name} ({content['lines']} è¡Œ)")
            else:
                # è¿™æ˜¯ä¸€ä¸ªç›®å½•
                print(f"{prefix}{current_prefix}ğŸ“‚ {name}/")
                next_prefix = prefix + ("    " if is_last_item else "â”‚   ")
                print_tree(content, next_prefix, is_last_item)
    
    print_tree(tree)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æœ¬åœ°çŸ¥è¯†åº“æ¼”ç¤º")
    print("=" * 50)
    print("åŸºäºLangChainçš„æœ¬åœ°æ–‡æ¡£çŸ¥è¯†åº“ç³»ç»Ÿ")
    print("æ–‡æ¡£æ¥æº: docs/ ç›®å½•")
    
    # åŠ è½½æ–‡æ¡£
    documents = load_all_documents()
    
    if not documents:
        print("\nâŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
        print("è¯·ç¡®ä¿docsç›®å½•å­˜åœ¨ä¸”åŒ…å«.mdæ–‡ä»¶")
        return
    
    print(f"\nâœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    
    # åˆ†ææ–‡æ¡£
    stats = analyze_documents(documents)
    
    print("\nğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯:")
    print("-" * 30)
    print(f"ğŸ“„ æ–‡æ¡£æ€»æ•°: {stats['total_docs']}")
    print(f"ğŸ“ æ€»å­—ç¬¦æ•°: {stats['total_size']:,}")
    print(f"ğŸ“ æ€»è¡Œæ•°: {stats['total_lines']:,}")
    print(f"ğŸ“ å¹³å‡å¤§å°: {stats['avg_size']:,} å­—ç¬¦")
    print(f"ğŸ“‹ æœ€å¤§æ–‡æ¡£: {stats['largest_doc']['path']} ({stats['largest_doc']['size']:,} å­—ç¬¦)")
    
    print("\nğŸ“‚ ç›®å½•åˆ†å¸ƒ:")
    for dir_name, info in stats['directories'].items():
        print(f"  {dir_name}: {info['count']} ä¸ªæ–‡æ¡£, {info['size']:,} å­—ç¬¦")
    
    # æ˜¾ç¤ºæ–‡æ¡£æ ‘
    show_document_tree(documents)
    
    # æ¼”ç¤ºæœç´¢åŠŸèƒ½
    print("\nğŸ” æœç´¢åŠŸèƒ½æ¼”ç¤º:")
    print("=" * 50)
    
    demo_queries = ["API", "è´¦æˆ·", "åŒºå—", "æ™ºèƒ½åˆçº¦", "å…±è¯†"]
    
    for query in demo_queries:
        print(f"\nğŸ” æœç´¢: '{query}'")
        results = simple_search(documents, query, max_results=3)
        
        if results:
            print(f"   æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£:")
            for i, result in enumerate(results, 1):
                print(f"   {i}. ğŸ“„ {result['path']} ({result['matches']} æ¬¡åŒ¹é…)")
                if result['snippets']:
                    snippet = result['snippets'][0][:100] + "..." if len(result['snippets'][0]) > 100 else result['snippets'][0]
                    print(f"      > {snippet.replace(chr(10), ' ')}")
        else:
            print("   âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
    
    print("\n" + "=" * 50)
    print("ğŸ’¡ å®Œæ•´çš„LangChainçŸ¥è¯†åº“åŠŸèƒ½:")
    print("   â€¢ å®‰è£…ä¾èµ–: pip install -r requirements.txt")
    print("   â€¢ é…ç½®APIå¯†é’¥: ç¼–è¾‘ .env æ–‡ä»¶")
    print("   â€¢ æ„å»ºçŸ¥è¯†åº“: python build_knowledge_base.py")
    print("   â€¢ å¯åŠ¨Webç•Œé¢: streamlit run app.py")
    print("   â€¢ æˆ–ä½¿ç”¨å¯åŠ¨è„šæœ¬: python start.py")
    
    print("\nğŸ¯ ä¸»è¦åŠŸèƒ½:")
    print("   â€¢ æ™ºèƒ½å‘é‡æœç´¢")
    print("   â€¢ AIé—®ç­”ç³»ç»Ÿ")
    print("   â€¢ Webäº¤äº’ç•Œé¢")
    print("   â€¢ æ–‡æ¡£è¯­ä¹‰ç†è§£")
    
    print("\nğŸ‘‹ æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    main()